---
title: vector-databases-and-the-power-of-rag
description: In this episode of the AI + a16z podcast, Pinecone Founder and CEO Edo Liberty joins a16z’s Satish Talluri and Derrick Harris to discuss the promises, challenges, and opportunities for vector databases and retrieval augmented generation (RAG). He also shares insights and highlights from a decades-long career in machine learning, which includes stints running research teams at both Yahoo and Amazon Web Services.
---

# vector-databases-and-the-power-of-rag

It soon became apparent that there are deeply intriguing questions to be answered in the field. The main challenge, however, lies in the complexity of these questions; classical tools for analyzing machine learning are inadequate for the task at hand. It is akin to having studied individual grains of sand when we now need to understand the behavior of entire dunes—a completely different phenomenon. Hello, this is Derek, and you're listening to the A16Z AI podcast where we explore all aspects of artificial intelligence with our in-house team of experts, as well as founders, engineers, and researchers at the cutting edge. In this episode, I am joined by A16Z partner Satish Tully and Pine Code founder and CEO Edo Liberty. Pinecone operates a vector database, and we will delve into this technology and its applications beyond just AI generation. Edo has been involved in machine learning for approximately two decades and has led research teams at AWS and Yahoo since 2009.

不久，我们明白了这个领域有许多深刻而有趣的问题需要解答。然而，主要的挑战在于这些问题的复杂性；传统的机器学习分析工具已不再适用。这就像是之前我们研究沙粒，而现在需要了解整个沙丘的行为—这是一个全然不同的现象。大家好，我是Derek，欢迎收听A16Z人工智能播客，我们将与我们的内部专家团队以及处于前沿的创始人、工程师和研究人员探讨人工智能的各个方面。在这一集中，我和A16Z合伙人Satish Tully以及Pine Code的创始人兼CEO Edo Liberty一起，探讨Pinecone经营的向量数据库及其除了人工智能生成之外的应用。Edo在机器学习领域已有大约二十年的经验，并从2009年开始在AWS和雅虎领导研究团队。


You might also consider this a journey through the history of machine learning. As a reminder, please be aware that the content provided here is solely for informational purposes. It should not be construed as legal, business, tax, or investment advice, nor should it be used to assess any investment or security. Furthermore, it is not intended for any investors or potential investors in any A16Z fund. For additional information, please visit a16z.com/disclosures. Initially, I began exploring this subject during my Ph.D., which started in 2003. At that time, the fundamental theories of machine learning were being thoroughly formalized. The study of machine learning predates this, having begun earnestly in the 1970s when the community started to integrate concepts from optimization, functional analysis, online algorithms, and even graph theory. These disciplines have significantly contributed to our understanding of how machines can learn. Ultimately, our focus remains on enhancing machine intelligence — enabling them to learn, remember, react, and predict effectively.

也可以把这看作是一次机器学习历史的回顾之旅。请注意，此处内容仅供信息参考，不应被视为法律、商业、税务或投资建议，亦不应用于评估任何投资或安全性，而且这些信息不针对任何A16Z基金的投资者或潜在投资者。更多详细信息，请访问a16z.com/disclosures。最初，我在2003年开始的博士研究期间，深入探讨了这一主题。那时，机器学习的基础理论正在被彻底系统化。机器学习的研究实际上早在70年代就开始了，当时学术界开始整合优化、功能分析、在线算法甚至图论等概念。这些学科显著促进了我们对机器如何学习的理解。最终，我们的重点仍然是增强机器智能——使机器能够学习、记忆、反应并有效预测。


On the set, the only techniques were mathematics and basic coding. There was no advanced support available; if you created a new model, you had to manually build the gradient descent, optimization processes, and everything else from scratch. There were no tools like TensorFlow or auto differentiation; everything had to be done manually. We were deeply focused on resolving the same issues: how to enhance processing speed, improve accuracy, and train larger models more effectively. This focus has persisted throughout my entire career. We've witnessed substantial improvements in every aspect of this problem, thanks to the collective efforts of countless exceptional and diligent scientists and engineers. For instance, consider what constituted a big model in 2003. To give you an example, driven by the challenges I encountered in the field, I delved into dementia reduction and big data while working on hyperspectral microscopy.

在拍摄现场，我们使用的技术只包括数学和基础编程，并无高级支援。若开发新模型，需要手动构建梯度下降、优化过程等一切内容。当时没有像TensorFlow这样的工具或自动微分功能，一切操作都必须手动执行。我们深入探讨的问题始终相同：如何提高处理速度，提升数据准确性，以及更有效地训练更大的模型。这种关注持续贯穿了我整个职业生涯。多亏了众多非凡且勤奋的科学家和工程师的集体努力，我们在这一问题的每个方面都见证了巨大的改进。例如，2003年的“大模型”是什么？我可以举一个例子，当时我从事的高光谱显微镜工作促使我深入研究痴呆缩减和大数据领域。


You can analyze images beyond the RGB spectrum using highly specialized microscopes capable of capturing hundreds of spectral data—essentially varying wavelengths of light. This technology significantly enhances the ability to discern different chemicals and structures in the images. For instance, such advancements are crucial in areas like cancer cell detection. Previously, each image contained several hundred spectra, each corresponding to a different type of light, resulting in files sized in the tens of megabytes. Consequently, the cumulative data could easily reach several gigabytes, making it impossible to store even a single image in the memory of a typical desktop computer, which back then might only have had 64 MB of RAM. The challenge was so considerable that we could not process a single image, let alone thousands, from one experiment. This issue highlights a major shift in the fields of artificial intelligence and machine learning, regarding both the algorithms used and the overall scale of data processing.

您可以使用高度专业的显微镜分析RGB光谱之外的图像，这种显微镜能够捕获数百种光谱数据——本质上是不同的光波长。该技术显著提高了在图像中识别不同化学物质和结构的能力。例如，在检测癌细胞等领域，这类技术的进步至关重要。以前，每张图片包含数百个光谱，每个光谱对应不同类型的光，文件大小达数十兆字节。因此，累积数据很容易达到几个千兆字节，使得无法将单个图像存储在典型的台式计算机内存中，当时的计算机可能只有64 MB的RAM。这一挑战非常显著，以至于我们无法处理单个图像，更不用说来自一个实验的成千上万个图像了。此问题凸显了人工智能与机器学习领域在使用的算法及数据处理规模方面的重大转变。


The discussion highlights the significant role of infrastructure, such as compute resources, in the field. The speaker credits much of their career success to fortunate circumstances, noting that many innovations were developed out of necessity to manage costs and enhance efficiency and speed. In the early stages, there was a need to maximize the minimal available resources, which involved optimizing data compression and processing techniques to make projects feasible on limited hardware. This approach of doing more with less continued with the transition to cloud computing. Although this shift offered seemingly unlimited resources, it came with its own challenges as computational power is not without cost. The skills and passion that drove early practitioners in machine learning to innovate under constraints are still applicable today, as they aim for efficiency in a new technological context.

讨论强调了基础设施，如计算资源，在该领域中的重要作用。发言者将其职业生涯的许多成功归功于幸运的机遇，指出许多创新都是为了管理成本以及提高效率和速度而开发的。在早期阶段，有必要在有限的硬件上最大化可用资源，涉及优化数据压缩和处理技术，使项目在有限的硬件上可行。这种在资源有限的情况下力求做得更多的方法随着向云计算的过渡而继续。虽然这种转变提供了看似无限的资源，但它自身也带来了挑战，因为计算能力并非没有成本。驱动早期机器学习从业者在限制条件下创新的技能和热情如今仍然适用，因为他们在新的技术环境中寻求效率。


We need the same resources not because we are incapable of running those workloads, but because we seek to reduce our expenses by tenfold compared to adopting a naive approach. This opportunity arose purely by chance, by being at the right place at the right time and acquiring skills that, unexpectedly, became crucial two decades later. Reflecting on your time at Yahoo, it's interesting to note that Yahoo was a key influencer in the development of Hadoop and greatly contributed to its ecosystem, which included a machine learning stack. This might be particularly pertinent to those who worked at Yahoo. Over a decade ago, the machine learning stack at Yahoo looked quite impressive. People often forget this, but Yahoo played a pivotal role in the machine learning ecosystem. At nearly every academic conference related to information retrieval or machine learning, such as KDD, SIGIR, NeurIPS, or others at that time, Yahoo's contributions were evident in approximately 20 to 30% of all papers presented.

我们需要相同的资源，并不是因为我们无法运行这些工作负载，而是因为我们希望比采用初级方法开支少十倍。这种机会纯属偶然，恰好处于正确的地点和时间，并习得了二十年后意外变得至关重要的技能。回想在雅虎的时光，雅虎在Hadoop的发展和生态系统的构建中起了关键作用，这一点尤其与在雅虎工作的人员相关。十多年前，雅虎的机器学习技术栈非常引人注目。人们常常忘记，雅虎在机器学习生态系统中扮演了枢纽角色。几乎在每一场与信息检索或机器学习相关的学术会议上，比如KDD、SIGIR、NeurIPS等，那时雅虎的贡献可见于大约20%到30%的论文中。


At least one Yahoo author played a pivotal role in nurturing an immense depth of talent, led by Prabakar Van, who is now among the highest-ranking executives at Google. During that era, a significant amount of innovation occurred at Yahoo, contributing to the development of Hadoop and various core principles and libraries that underpin much of today's machine learning, OLAP, and additional databases. Early versions of technologies now integral to systems like Kubernetes were also conceived at Yahoo. Being a part of that innovative culture, collaborating with an exceptional group of individuals, and directly contributing to the development of these technologies was an immensely fulfilling experience. Over time, advancing within the organization and leading numerous projects related to machine learning infrastructure at Yahoo was an extraordinary privilege—definitely a combination of serendipity and intense effort. Indeed, during that period, the primary focus was predominantly on search and information retrieval, which were considered the major categories within the organization.

在Yahoo，至少有一位作者曾在Prabakar Van的领导下，对人才的深层开发起到了关键作用，他如今是Google的高级领导之一。在那个时代，Yahoo发生了大量创新活动，不仅开发了Hadoop，还构建了支撑当今大部分机器学习、在线分析处理(OLAP)及其他数据库的核心原则和库。像今天的Kubernetes等系统所依赖的早期技术，同样也是在Yahoo发明的。成为那种创新文化的一部分，与一群杰出的人共事，并亲手参与这些技术的开发，无疑是一种极大的满足感。随着时间的推移，在组织中不断上升，领导Yahoo机器学习基础设施的众多项目，是一种非凡的特权—绝对是偶然和努力的结合。实际上，在那段时间里，主要的焦点主要集中在搜索和信息检索上，这被视为组织中的主要类别。



During that period, there was indeed a very dedicated effort towards machine learning at Yahoo. Some of the most prominent thought leaders in both the theory and practice of machine learning were part of the Yahoo team. On my very first day at Yahoo, I was working on a project known as Vocal Rabbit, led by John Langford and his team. At that time, Vocal Rabbit was considered the definitive standard for linear regression and classification. It was an exceptionally optimized C library for these tasks, a creation only someone as ingenious as John Langford could envision and manage to maintain so comprehensively in his mind. The library incorporated every conceivable aspect that one could address with generalized linear classifiers. Machine learning played a significant role and was a key component in applications such as spam classification and prediction.


在那个时期，雅虎对机器学习确实进行了非常专注的努力。该领域理论与实践的一些顶尖思想领袖都在雅虎工作。我在雅虎的第一天就参与了一个名为Vocal Rabbit的项目，该项目由John Langford及其团队领导。当时，Vocal Rabbit被认为是线性回归和分类的权威标准。它是一个极其优化的C语言库，专门用于这些任务，只有像John Langford这样才智非凡的人才能构思出这样的项目，并且能够在他的脑海中完整地维护这一概念。这个库涵盖了人们可用于广义线性分类器的每一个可能的方面。机器学习在诸如垃圾邮件分类和预测等应用中起到了重要作用，是关键的组成部分。


Machine learning has driven significant advancements in areas such as search and ad placement. During a discussion at KDD around 2012 or 2013, deep learning was introduced as a concept that marked a substantial shift in the field. This shift involved changes in the types and sizes of models used, which significantly captured the zeitgeist and has continued to grow in influence. Initially, I was quite skeptical about deep learning due to my background in theoretical machine learning and computer science, where I focused on proving convergence, improving asymptotic running times, and understanding worst-case behaviors. Despite this, deep learning emerged as a practical approach where models were simply run, yielding interesting results that were hard to ignore even from a mathematician's perspective.

机器学习在搜索和广告投放等领域推动了显著的进展。在2012或2013年的KDD会议上，深度学习作为一个概念被介绍，并标志着该领域的一个重大转变。这种转变涉及到所使用的模型类型和大小的变化，这些变化显著地引起了时代精神的关注，并且其影响力持续增长。起初，由于我在理论机器学习和计算机科学领域的背景，我对深度学习持相当怀疑的态度，我的工作重点是证明收敛性，改善渐近运行时间，并理解最坏情况的行为。尽管如此，深度学习作为一种实用方法出现了，模型简单地运行并产生了有趣的结果，即使从数学家的角度来看，这些结果也难以忽视。


And so, for several years, the conversations between theoreticians and practitioners typically involved a theoretician asserting that something was impossible, while the practitioner countered by stating they had already accomplished it that morning. Despite repeated claims of impossibility by theoreticians, practitioners continued to demonstrate practical results. Over time, the theoretical community began to acknowledge that, although they could not fully understand or prove the principles behind these results, it was evident that the results were real and significant. Consequently, the focus shifted towards understanding and intelligently discussing how these phenomena work. For instance, efforts were made to analyze phenomena such as the absence of overfitting, which contradicted established VC theory. According to VC theory from the previous century, models with more parameters than the dataset size are typically expected to overfit, yet in practice, this was not always the case.

多年来，理论家与实践者之间的对话通常是这样的：理论家坚称某事是不可能的，而实践者则回应说他们当天早上刚刚实现了这一目标。尽管理论家一再声称这是不可能的，实践者却不断展示出实际成果。随着时间的推移，理论界开始承认，尽管他们无法完全理解或证明这些成果背后的原理，但显然这些成果是真实且重要的。因此，研究重点转向了如何理解和智能地讨论这些现象的运作方式。例如，开始尝试分析如无过拟合现象之类的现象，这与既有的VC理论相矛盾。根据上个世纪的VC理论，参数数量超过数据集大小的模型通常会出现过拟合问题，但实践中并非总是如此。


When it comes to creating an ideal model with your own data, it often fails dramatically when confronted with new, unseen scenarios. This wasn't the case with deep learning. Unlike previous assumptions, deep learning demonstrated robustness even when continuously trained on the same data. Instead of plateauing, the models continued to improve without generalizing successfully. This intriguing phenomenon underscored the need for a deeper understanding of the underlying mechanisms at play. For someone like myself, deeply involved in technology development, the turning point was profound. Initially, deep learning did not seem promising; it appeared as merely a combination of numerous shortcuts integrated by sophisticated software and enhanced by the speed of GPUs. It all seemed too familiar, just more of the same.


在利用自己的数据创造理想模型的时候，当面对全新未见过的情景时，这种模型常常会戏剧性地失败。然而在深度学习的情况下，并非如此。与先前的假设不同，深度学习即使在不断地对同一数据进行训练时仍显示出强大的鲁棒性。模型没有如预期的那样停滞不前，反而持续改进，但并未成功泛化。这种引人注目的现象强调了需要更深入地理解其中的运作机制。作为一名深度投身于技术开发的人，这一转变点是深刻的。起初，深度学习并没有显示出令人印象深刻的成果；它看起来仅仅是由高级软件和GPU加速整合在一起的众多快捷方法的组合。这一切似乎过于熟悉，只是更多的相同内容。


It took some time to realize the uniqueness of this matter. Although I can't pinpoint the exact year, it soon became clear that there were several profound and intriguing questions that needed answers. The main challenge, however, lies in the complexity of these questions. The conventional tools used for analyzing classical machine learning proved inadequate for these new challenges. It's akin to having studied individual grains of sand when what is required is an understanding of the behavior of entire sand dunes. Theoretically, they might appear similar, but in reality, they represent entirely distinct phenomena. Subsequently, I joined the Amazon SageMaker team, where I had the opportunity to build the team and develop the machine learning practice. The challenges faced there became substantially more tangible and, correspondingly, more thrilling for someone like me. It involved devising solutions that catered not just to individual needs but were also viable for widespread use by engineers.

花了一些时间我才真正意识到这件事的特别之处。虽然我不太记得具体的年份，但很快就明显地注意到有一些深刻且有趣的问题需要解答。然而，主要的挑战在于这些问题的复杂性。用于分析传统机器学习的常规工具已不再适用。这就好比我们之前仅研究沙粒，现在却需要理解沙丘的行为。理论上，它们似乎相似，但实际上是完全不同的现象。随后，我加入了亚马逊的SageMaker团队，在那里我负责建立团队并发展机器学习实践。那里的挑战变得更加实际，对我这样的人来说也更加令人兴奋。工作涉及的不仅是满足个别需求，而是为工程师们提供广泛使用的解决方案。


Product development, particularly when aiming for a consumption product, demands supreme simplicity and predictability in its use, ensuring that even those unfamiliar with its optimizations can understand and operate it easily. Nevertheless, internally, the approach shifts more towards service creation, involving the management and real-time operation of the system, along with overseeing operations and budgeting. This multi-layered complexity added to product development was a revelation to me. For example, in today’s discussions around pricing, packaging, and user accessibility features such as flag options, I drew comparisons between companies like Yahoo and AWS. It highlighted the distinctions in handling an open-source software scenario, where, as a developer within a company, one should consider whether to make such features openly available.

产品开发，尤其是面向消费产品的开发，要求产品使用的极度简单和可预测性，确保即使是不熟悉产品优化的用户也能轻松理解和操作。然而，在内部，开发过程更多地转向了服务的创建，涉及到系统的管理和实时运行，以及操作和预算的监督。这种多层次的复杂性为产品开发带来了启示。例如，在今天关于定价、包装和用户可用功能（如标志选项）的讨论中，我对比了Yahoo和AWS等公司的不同处理方式。这突显了在开源软件场景中，作为公司内部的开发者，是否应该开放此类功能的重要考虑。




If you operate a managed service, you should not opt for open sourcing your software. The reason lies in the irreversible nature of this decision. Once a software begins its life as open source, it remains open indefinitely. Consequently, managing this software becomes an exceedingly challenging task. For managed databases like ours, open sourcing offers no tangible benefits. It is akin to giving away the blueprint for building a car instead of selling the car itself, which is unhelpful. Pine Cone was launched in 2019 with a deliberate vision. The decision to initiate Pine Cone was influenced by the apparent potential in vector databases. By that time, embeddings and vector usage in various applications such as search engines, recommendation systems, semantic searches, and feed ranking in shopping were becoming increasingly significant. This trend, which we had been exploring for about a decade, was gaining substantial traction.



如果您运营一项管理服务，您不应选择开源您的软件。原因在于这个决定的不可逆性。一旦软件以开源方式问世，它将永远保持开放。因此，管理这种软件变得极其困难。对于像我们这样的管理数据库来说，开源并没有实际的好处。这就像是提供制造汽车的蓝图，而不是直接销售汽车本身，这并无帮助。Pine Cone在2019年启动时有着明确的视野。启动Pine Cone的决定受到了向量数据库潜力的启发。那时，嵌入式技术和向量在搜索引擎、推荐系统、语义搜索以及购物排名等多种应用中的使用日益增长。这一趋势，在过去十年中我们已进行了深入探索，正快速获得实际推广。


At the same time back then, the first truly open-source pretrained model was released, known as BERT. Despite being slow, clunky, and often embarrassingly incorrect and ineffective in many different ways, it represented the first significant breakthrough. Occasionally, BERT would produce outputs that were surprisingly effective, prompting observers to recognize a potential hidden gem within. This sparked a competitive pursuit of advancements in AI among major companies like Facebook, Amazon, and Google. Non-AI enthusiasts, including engineers curious about AI technologies, began to actively discuss concepts such as embeddings and vector search. Initially, I felt that it was far too soon for these discussions; no one seemed to understand what I was talking about and it appeared utterly preposterous. However, merely hours later, it seemed everyone was aware of it, making me think I was too late to the conversation.

在那段时间，第一个真正的开源预训练模型BERT被发布，尽管其运行缓慢、笨拙且经常出错，在许多方面无用，但它象征着一个重大的突破。BERT偶尔能产生出人意料的有效输出，使观察者们认识到其潜在的价值。这激发了包括Facebook、亚马逊和谷歌在内的主要公司之间竞争AI技术的进步。对AI技术感兴趣的非AI专业人士，包括工程师们开始积极讨论嵌入和向量搜索等概念。起初，我觉得讨论这些话题为时尚早，没有人似乎理解我在说什么，这看起来完全是荒谬的。然而，仅几小时后，似乎每个人都已经知晓，让我觉得我反而来得太迟。




Indeed, starting three years earlier might have been beneficial, but continual difficulties might actually suggest it was not the correct choice. This is certainly indicative of good intuition. I recall our initial meetings when the primary applications were centered around semantic search and recommendation engines. Netflix and Amazon exemplified this, with Netflix using it to power suggestions and Amazon for showing similar items. Speaking from firsthand experience, semantic search is still a key application for us, as we are gradually replacing other solutions like Elasticsearch. Companies are increasingly adopting semantic data representations due to the advantages in scalability, speed, and cost. Moreover, most AI startups focus on either developing a model or enhancing search capabilities. In fact, nearly 95% of startups today are engaged in one of these two activities.

Meaningful Translation in Chinese:

确实，如果三年前就开始可能会更好，但不断出现的问题可能实际上表明这不是正确的选择。这无疑是一个良好的直觉的体现。我记得我们早期的会面，当时的主要应用是围绕语义搜索和推荐引擎展开的。Netflix和Amazon是两个例子，Netflix用它来推动建议，而Amazon用来显示相似的商品。根据我亲身的经历，语义搜索仍然是我们的一个主要应用，我们正在逐步取代Elasticsearch等其他解决方案。由于可扩展性、速度和成本的优势，越来越多的公司正在采用语义数据表示。此外，大多数AI初创公司专注于开发模型或改进搜索功能。实际上，如今大约95%的初创公司都在进行这两项活动之一。


The journey has essentially been an extrapolation from the start. The scaling of recommendations did not keep pace with Rag's advances in semantic search, however, Rag significantly accelerated the development of semantic search. A clear contrast helps in understanding its impact. Previously, a vector database was employed to retrieve semantically similar documents, passages, or facts placing them into a context. In terms of search results consumption by a human, there are limits to processing capacity — typically handling up to 10 blue links, paragraphs, or a page of content. With Rag, it becomes possible to process the results of up to 100 full documents from a search. These documents can then be further analyzed using a language or foundational model to extract specific pieces of information or outputs.


这个过程实际上从一开始就是一个外推。虽然推荐的扩展速度没有跟上Rag在语义搜索方面的进步，但Rag显著地加速了语义搜索的发展。明显的对比有助于理解其影响。以前，我们使用向量数据库检索在上下文中语义上相似的文档、段落或事实。但从人类的角度来看，处理搜索结果的能力是有限的——通常只能处理最多10个蓝色链接、段落或一页内容。通过Rag，我们可以处理来自搜索的多达100个完整文档的结果。然后可以利用语言模型或基础模型进一步分析这些文档，以提取特定的信息或输出。


New technologies such as Rag have opened up an extensive range of opportunities, particularly where traditional semantic search technologies proved insufficient. Beneath the surface, the primary mechanism remains vector search, which is crucial for searching with efficiency, scalability, and cost-effectiveness. The introduction of Large Language Models (LLMs) as a second phase in place of human intervention marks a significant shift, making a new spectrum of applications feasible, such as question and answer systems. Currently, many companies, some developing solutions in-house—though predominantly those able to invest substantially—are those creating their products. These are primarily AI companies that develop Rag products collaboratively with us, bringing both expertise and motivation to develop highly effective Rag solutions relevant to their needs. The potential to then market these solutions extensively is enormous. This represents, indeed, a significant transformation. Just this morning, I came across a discussion and would appreciate personal enlightenment on the topic, which revolves around a specific debate.



新技术如Rag为我们开辟了广泛的新机会，尤其是在传统的语义搜索技术表现不足的领域。其底层机制依旧是向量搜索，这对于实现高效、可扩展及成本效益的搜索至关重要。引入大型语言模型（LLM）作为第二阶段的处理者，而非人类，这一变化标志着重大转变，使得实现新的应用场景如问答系统等成为可能。目前，许多公司，有些是内部开发解决方案的—尽管主要是那些能够大量投资的公司—正在创建他们自己的产品。这些主要是与我们合作开发Rag产品的AI公司，他们不仅拥有专业技术，也有强烈的动机去开发符合他们需求的高效Rag解决方案。这些解决方案的市场潜力巨大，可以广泛地销售。这确实代表了一个重大的变革。今天早上我刚读到一个讨论，希望对这个话题有更深的了解，这个讨论主要围绕一个特定的辩论展开。


The discussion centers on whether utilizing larger or more extended contextual windows improves the performance of models, or if Reg, presumably a method or technology, could be the solution. The query asks whether this is an exclusive choice or if both elements can coexist. There is a clear stance that it is not a matter of choosing one over the other. If it is feasible to analyze a broader context effectively and cost-efficiently, then this approach is preferable as it enables the search engine to deliver more relevant data, enhancing overall performance. However, this does not diminish the need to retrieve the most pertinent information from the available sources. For instance, if the information required is located in one document, then providing that document in the context is possible. If the context extends to ten documents, perhaps all ten can be provided. However, if the volume reaches a million or more documents, such an approach becomes impractical and cannot be employed. Therefore, while expanding context windows is beneficial where possible, it is not always feasible with extremely large data sets.

讨论集中于使用更大或更长的上下文窗口是否能提升模型的表现，或者Reg（可能是一种方法或技术）是否为改进的答案。问题在于这是一个选择其一的提议，还是两者可以共存。明确的观点是这并不是选择其一的问题。如果能够有效且经济地分析更广泛的上下文，那么这种方法更为可取，因为它使搜索引擎能够提供更相关的数据，从而提升整体性能。然而，这并不减少从可用来源检索最相关信息的需求。例如，如果需要的信息位于一份文档中，提供该文档的上下文是可行的。如果上下文扩展到十份文档，也许可以提供全部十份。但是，如果数据量达到一百万份或更多，这种方法就变得不切实际，无法使用。因此，虽然在可能的情况下扩大上下文窗口是有益的，但在极大的数据集中并不总是可行。


The concept of context is crucial; it significantly enhances the performance and accuracy when integrated with queries. Analogously, sending a query to a search engine without contextual information is like sending the entire internet along with it. This exaggerated scenario highlights the potential inefficiencies of providing excessive context, ultimately suggesting a more balanced approach. A system that intelligently uses the most relevant context leads to more efficient and precise results. Accuracy is paramount, particularly when considering the financial model where vendors charge per token. Thus, vendors are eager to suggest that more context equates to better results. It's crucial, however, to strike the right balance to ensure performance and accuracy without unnecessary cost.

上下文的概念非常关键，当其与查询信息一起被整合使用时，能显著提高性能和准确性。类比地，将一个查询发送到搜索引擎但不提供上下文信息，就像连同整个互联网一起发送一样。这种夸张的场景突出显示了提供过多上下文可能带来的效率低下，最终表明了需要一个更平衡的方法。一个智能使用最相关上下文的系统会导致更高效和精确的结果。准确性是至关重要的，尤其是在考虑到按令牌计费的金融模型。因此，供应商急切地建议更多的上下文等于更好的结果。然而，关键是要找到正确的平衡，以确保在不增加不必要成本的情况下提高性能和准确性。


It is widely recognized that injecting irrelevant information into the context, an issue known as context stuffing, does not aid understanding. In fact, it not only fails to provide assistance due to its irrelevance but also complicates matters by introducing confusion and distraction. Moreover, such practices are not merely wasteful; they actually impair results. Thus, it is crucial to supply the appropriate context to the model, which significantly impacts both accuracy and cost efficiency. This aspect becomes even more critical when considering scaling. With unit economics coming into play, working with large-scale customers, for whom operating major applications can justify substantial investments, becomes evident. While spending 100,000 dollars might be easily justified, and even one million dollars might be considered feasible, a ten-million-dollar expenditure remains decidedly prohibitive. Additionally, the fundamental infrastructure is evolving; the procedure is no longer as straightforward as merely segmenting documents into vectors, storing them in a vector database, and retrieving the top relevant documents for the user upon request.

广泛认为向上下文中添加不相关信息（这一问题称为上下文填充）无助于理解。事实上，这不仅因为其不相关性而无法提供帮助，还因为它引入了混淆和分散注意力的因素，使情况变得更加复杂。此外，这种做法不仅是浪费资源，实际上还会损害结果。因此，为模型提供适当的上下文至关重要，这对精确度和成本效率有显著影响。当考虑到规模扩展的时候，这一点变得更为重要。开展与大规模客户的合作显而易见，对于他们来说，运行大型应用程序可以证明大额投资是有道理的。虽然花费100,000美元可能很容易被接受，甚至100万美元也可能被视为可行，但1000万美元的开支绝对是不可行的。此外，基础架构也在发展中；过程不再像以前那样简单，只需将文档分割成向量，存储在向量数据库中，然后在用户请求时检索最相关的顶部文档即可。


To summarize, that's the basic RAG. However, as these apps move to production, they evolve significantly. Users are implementing complex chunking strategies and employing rerankers following vector database queries, among other advanced techniques. What trends are emerging among your customers as they move from rudimentary RAG solutions to sophisticated systems in production? Give us an overview of the prevailing trends. I'll start by noting that RAG today is reminiscent of the state of transformers in 2017. It's somewhat clunky, unusual, and challenging to perfect, harboring many pitfalls. Yet, it already performs amazingly in many cases. The earliest adopters and the most advanced users have already embraced it and are adept at navigating its complexities. RAG represents a broad paradigm, involving aspects like chunking, model encoding, choices and configurations of vector databases, and their setup.

总结来说，这就是基本的RAG。但是，当这些应用程序投入生产时，它们在不断发展。使用者正在实施复杂的块处理策略，并在向量数据库查询之后使用一些重排序器等先进技术。请描述一下您的客户群从初级的RAG解决方案到生产中的复杂高级系统之间的趋势变化。请向我们介绍一下主要的趋势。我可以说，今天的RAG与2017年的变换器有些相似，它稍显笨拙、不同寻常且难以完善，存在许多陷阱。然而，它在许多情况下已经表现出惊人的性能。最早的采用者和最先进的用户已经开始接受并熟练地处理其复杂性。RAG代表了一个广泛的范式，涉及块处理、模型编码、向量数据库的选择和配置以及它们的设置。



In considering the complexity of handling results, re-ranking, pruning, reordering, context, prompting, and model choice, multiple decisions need to be made, along with the construction of various subsystems. It is evident that those who make significant advancements in this field are individuals who embrace the challenges and understand that perfection is unattainable on the first attempt. Assuming many aspiring founders or current founders are tuning in, advancing in areas such as relevance assessment, information retrieval, enhancing AI's knowledge base, reducing superficiality, and increasing reliability, represents a substantial opportunity for innovation today. Currently, companies address these challenges in-house. Regarding the transition to production, organizations typically rely on tools like Pinecone or other model providers, and must carefully select the most reliable partners and infrastructure for production deployment.


在处理结果、重新排名、修剪、重新排序、上下文、提示及模型选择等复杂问题时，需要做出多项决策并构建各种子系统。在这个领域取得显著进步的人往往是那些能够接受挑战并理解首次尝试不可能完美的人。假设很多有意成为创始人或现任创始人正在关注此事，那么在相关性评估、信息检索、增强AI的知识库、减少表面文章、提高可靠性等方面取得进展，今天代表了一个巨大的创新机会。目前，公司通常自行解决这些挑战。关于过渡到生产阶段，组织通常依赖于Pinecone或其他模型提供者等工具，并且必须仔细选择最可靠的合作伙伴和生产部署基础设施。


They build in-house knowledge on how to assess and create better solutions. Our company closely partners with them, investing in their success because it also equates to our success. The highest correlation between those who reach production successfully are those committed to making things work. While the process is rarely as glamorous as desired, it is effective and leads to significant accomplishments. Our customers and partners who progress to production typically see substantial benefits in adoption and customer experience, despite their systems being far from perfect. As engineers and technical professionals, we always strive for improvements, seeking better, faster, cheaper, and more accurate systems.

他们建立了内部知识，以评估和创建更好的解决方案。我们公司与他们密切合作，共同投资于他们的成功，因为这同样也是我们的成功。成功进入生产阶段最明显的关联是那些致力于使事情发挥作用的人。虽然这个过程很少像人们希望的那样富有魅力，但它确实有效，并且取得了重大成就。我们的客户和合作伙伴在推进到生产阶段通常会看到在采用和客户体验方面的显著收益，尽管他们的系统远非完美。作为工程师和技术专业人员，我们始终寻求改进，追求更好、更快、更便宜、更准确的系统。


When customers encounter the technology, it seems magical. You’re describing an early, greenfield stage. So, what's the appeal to customers? Essentially, they should consider implementing a vector database and a rag, despite its imperfections. It’s unavoidable. For instance, my young children interact with every screen and speak to every device. If something isn't a touchscreen, they become puzzled. The landscape of software is evolving; users now expect to interact with software using natural language. This is particularly true for applications involving knowledge, documents, and images. The notion of having to code or navigate cumbersome processes such as selection, tagging, and the like has become intolerable. Users anticipate that the software should be sufficiently intelligent to comprehend their instructions; understanding spoken requests is fast becoming standard. People have this expectation from their software.

当客户遇到这项技术时，他们感受到了某种魔力。您描述的是一个处于早期的、未开发的状态。那么，对客户的吸引力是什么呢？基本上，他们应该考虑即使在存在缺陷的情况下也要实施向量数据库和rag。这是不可避免的。例如，我的年幼孩子会触摸每一个屏幕并对每一个设备说话。如果某物不是触摸屏，他们就会感到困惑。软件的发展正在发生变化；用户现在期望能够用自然语言与软件进行交互。这一点对于涉及知识、文档和图像的应用尤为真实。必须编码或者经历复杂的过程，如选择、标记等，已变得难以忍受。用户希望软件能够足够智能，以理解他们的指令；软件理解口头请求正迅速成为标准。人们对他们的软件有这样的期望。


Whether you represent an early-stage company or startup, understanding and mastering a niche within the software industry can offer vital capabilities to be sold to larger corporations. Alternatively, as a more established business being challenged by smaller competitors, who despite your reservation, are perceived by customers as capable. This perception pressures you to innovate to avoid disruption. Or perhaps you are a larger corporation asserting dominance and taking a leadership role in key innovative sectors like AI, asserting that mastery in areas such as AI is as essential as expertise in cloud technologies or cybersecurity, highlighting that investment in such areas is non-negotiable. It’s highlighted that while venturing into new technologies might currently seem challenging, waiting for a simpler solution might not be advantageous as technological advancements are continual.

无论您是刚起步的公司还是初创企业，在软件行业的一个子领域内掌握技术并能向大型企业出售此技能是至关重要的。另一方面，如果您是一个已经被小型竞争对手挑战的成熟企业，尽管您不信任这些竞争对手，但客户认为他们有能力，这迫使您需要创新以避免被颠覆。或者您可能是一个大型公司，正在主张自己的主导地位，并在关键的创新领域如人工智能上取得领导地位，您声称在人工智能等领域的掌握和精通像云技术或网络安全一样至关重要，强调在此类领域的投资是无可非议的。这还强调，虽然涉足新技术当前看似有挑战，但等待更简单的解决方案可能并不利，因为技术的进步是持续的。


The sentiment that advanced tools are improving is widely accepted; nonetheless, it is anticipated that expectations will correspondingly increase. Instituting such capabilities within an organization is crucial and should have commenced previously. Despite the current exorbitant costs and scarcity of AI talent, pursuing this skill set is not optional. AI expertise appears to be more widespread now compared to the past. Unlike a decade ago, when AI specialists were as rare and highly paid as NFL quarterbacks, today there seems to be a more general participation from hacking enthusiasts and individual developers. This appears to be a democratization of skills, though the term may be contentious. This shift raises questions about the implications for customers and companies like Pine Cone, especially in terms of recruiting capable employees.


当前的共识是，先进的工具正在改善，同时预计对它们的期望也会相应提高。在组织内部建立这种能力是至关重要的，而且应该早已开始着手进行。尽管目前获取人工智能（AI）人才成本高昂且较为困难，但这是一个必须面对的选择。与十年前相比，AI专家曾稀缺且薪酬可与NFL的四分卫相媲美，现在看来技能已经在更广泛的人群中传播，有更多的爱好者和独立开发者参与进来。这种变化似乎是一种技能的民主化，虽然这个词可能有争议。这种转变引发了关于对客户及像Pine Cone这样的公司在招聘能够真正构建这些技术的员工时会产生什么影响的问题。


I'm not in the process of hiring cooks; rather, I am seeking stove designers—AI talent that understands how to operate the tools. This type of expertise is becoming increasingly prevalent, which is a positive development. It is indeed exciting and allows for the creation of highly impressive technologies. However, this does not imply the feasibility of everyone going out to develop, for example, a new compiler for the parallel execution of language models on GPUs, as this remains prohibitively costly and challenging to acquire. That said, the emergence of such skills is noted, especially as they begin to be integrated more hands-on in both undergraduate and graduate curricula. Indeed, this kind of talent still comes at a high cost. From my perspective, even such specialized talent isn't as affordable as a typical engineer, although the term 'normal engineer' itself has become rather ambiguous. How do you perceive this impacting customer adoption or perhaps the speed at which concepts transition to production if the customers have staff who are adept with these tools?

我不是在招募厨师，而是在寻找炉灶设计师——懂得如何操作工具的AI人才。这种专长变得越来越普遍，这是一个积极的发展。这种情况确实令人激动，并能够创建非常令人印象深刻的技术。然而，这并不意味着每个人都可以去开发新的编译器，例如用于GPU上语言模型的并行执行的编译器，因为这仍然非常昂贵且难以获取。确实，这种技能的出现正在被更实践性地纳入本科及研究生教程中。的确，这类人才仍然非常昂贵。从我的角度来看，即使是这种专业人才也不像普通工程师那样“便宜”，尽管“普通工程师”的定义现在已经变得相当模糊。你如何看待这种情况如何影响客户的采纳，或者如果客户拥有擅长使用这些工具的员工，他们将概念转化为生产的速度可能会有多快？


Certainly! Here's the transformation based on the instructions provided:


It is absolutely critical that people understand how to use models, evaluate and interpret data, and think about development in an analytical mode. Having internal capabilities enhances an organization's ability to adopt the appropriate technological stack, whether that involves selecting the correct vector database, model provider, evaluation framework, or other complex choices. Our customers, who develop these systems often encounter specific issues. They approach us for advice, questioning the normalcy of their setups, and seeking guidance on potential improvements or concerns regarding their configurations. We are always ready to assist, though we cannot help everyone. We do our best, and naturally, the more skilled the in-house talent is, the faster and better they can build and innovate. Recently, we also launched our significant seller-less offering and introduced a hybrid search option.


了解如何使用模型、评估和解读数据以及以分析模式思考开发是极其重要的。拥有内部能力可以增强一个组织选择适当技术栈的能力，无论是选择正确的向量数据库、模型提供商、评估框架还是其他复杂的选择。我们的客户在开发这些系统时经常会遇到特定问题。他们找到我们寻求建议，询问他们的设置是否正常，以及寻求关于潜在改进或对其配置的担忧的指导。我们总是准备好提供帮助，虽然我们不能帮助每一个人。我们尽力而为，显然，越是拥有技术精湛的内部人才，他们构建和创新的速度及效果就会越快、越好。最近，我们还推出了我们重要的无卖家产品，并引入了混合搜索选项。


Are you observing any interesting trends among your customers in production? The most significant aspect is unit economics. For instance, operating a small application internally to learn how to use it might involve minimal costs. This small scale operation, perhaps running on a single machine and costing about 50 dollars a month, is generally inconsequential financially. However, for our large clients who manage services for hundreds or thousands of their own customers through our platform, cost becomes a critical factor. To drive down expenses substantially, we have developed a multi-tenant SaaS offering. This system allows us to control computing and resources with high precision. In this setup, when a query is executed, it utilizes 100 CPUs for just 10 milliseconds. After this time, these resources are reallocated to another user, and thus, the original user incurs no further cost. This method provides the necessary capacity and efficiency gains.


您是否在生产中观察到客户有什么有趣的趋势？其中最关键的是单位经济。例如，内部操作一个小应用以学习如何使用它可能涉及最小的成本。这种小规模操作，可能运行在单一机器上，每月成本约50美元，通常在财务上不具有重大影响。然而，对于通过我们的平台为成百上千甚至数千名自己的客户提供服务的大客户来说，成本成为一个关键因素。为了大幅降低费用，我们开发了一个多租户的SaaS产品。该系统允许我们高精度控制计算和资源。在这种设置中，执行一个查询时，它使用100个CPU仅仅10毫秒。这段时间过后，这些资源将重新分配给另一个用户，因此原用户不再产生任何额外成本。这种方法提供了所需的容量和效率增益。


We are currently collaborating with a significant customer who is considering deploying their free tier search service across many millions of customers. We demonstrated to them that the operational cost is exceptionally low, amounting to mere cents per year for each user. Specifically, operating this service for one user for an entire year costs approximately 4 cents—a feat achievable only through this method. Therefore, scale, ease of use, and unit economics are paramount when aiming to develop rapidly and maintain stability in production. What do you think is the next step, particularly concerning the direction enterprises will take with LMS and their implementation? We are on a mission to endow AI with knowledge, which involves not just reasoning and discussing topics but possessing deep understanding about the subjects being discussed.


我们目前与一家大型客户合作，该客户正在考虑将其免费层搜索服务推广至数百万用户。我们向他们展示了这种操作的成本极低，每个用户每年的成本仅为几美分。具体而言，为一个用户运行此服务全年的成本大约为4美分，这种低成本运作只有通过这种方式才能实现。因此，规模、使用便捷性与单元经济性在迅速开发并保持生产稳定时至关重要。您认为下一步是什么，特别是企业将如何推动LMS的发展和实施？我们的使命是使AI具备知识，这不仅仅是能够推理和讨论问题，更是要真正理解所讨论的主题。


We frequently discuss hallucinations as a mere problem; however, the issue is significantly more profound. Earlier discussions indicate that we are still in the preliminary stages of addressing this issue, and it is crucial that we continue to explore solutions. The Pine Cone community, along with various startups and businesses in the broader market, is actively attempting to tackle this challenge in specific domains or perhaps more broadly. It is clear that this direction will continue to be pursued. I am eager to see how quickly we, as a society not limited to Pine Cone but including everyone, can progress. It remains uncertain whether this challenging problem will take years to gradually solve or if it could be almost miraculously resolved within a year. The outcome will certainly be intriguing to observe. That concludes this week's episode. If you've listened to the end, consider rating us, subscribing, and sharing with your friends and colleagues—the usual actions that support the continuation of podcasts.

我们经常讨论幻觉仅仅是一个问题，但这个问题实际上更加深入。从之前的讨论来看，我们在解决这个问题的初期阶段，继续探索解决方案是至关重要的。Pine Cone社区以及更广泛市场中的多个创业公司和企业正积极尝试在特定领域或更广泛的范围内解决这一挑战。显然，这个方向将持续被追求。我很想知道我们作为一个社会（不仅限于Pine Cone，包括所有人）能多快取得进展。目前尚不清楚这个棘手的问题是需要几年时间逐渐解决，还是能在一年内几乎像魔法般地完全解决。其结果无疑非常值得期待。本周节目到此结束。如果您听到了这里，请考虑给我们评分、订阅并与您的朋友和同事分享——这些常规操作有助于播客的持续运作。


In the coming weeks, tune in as we explore a variety of topics, particularly focusing on the intersection of artificial intelligence and cybersecurity.

在接下来的几周里，请继续关注我们的节目，我们将探讨一系列话题，特别是人工智能与网络安全的交汇点。